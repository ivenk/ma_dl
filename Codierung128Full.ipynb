{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA_DL : Codierung 128bit (full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in /opt/anaconda3/lib/python3.7/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from facenet-pytorch) (1.17.4)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from facenet-pytorch) (2.22.0)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->facenet-pytorch) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->facenet-pytorch) (2019.11.28)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->facenet-pytorch) (2.8)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->facenet-pytorch) (1.24.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch # due to issues with the google cloud service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabel(nn.Sequential):\n",
    "    def __init__(self, input_dim=512, output_dim=128):\n",
    "        super(MultiLabel, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, input_dim)\n",
    "        self.l2 = nn.Linear(input_dim, input_dim)\n",
    "        self.l5 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l5(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabel(\n",
       "  (l1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (l5): Linear(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiLabel()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = transforms.Resize((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_image = transforms.ToPILImage()\n",
    "transform = transforms.Compose([\n",
    "    scaler, \n",
    "    to_tensor,\n",
    "#    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.ImageFolder('../../data/large_ds/train/', transform=transform)\n",
    "train_ds.idx_to_class = {i:c for c, i in train_ds.class_to_idx.items()}\n",
    "train_dl = DataLoader(train_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "valid_ds = datasets.ImageFolder('../../data/large_ds/valid/', transform=transform)\n",
    "valid_ds.idx_to_class = {i:c for c, i in valid_ds.class_to_idx.items()}\n",
    "valid_dl = DataLoader(valid_ds, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesichtserkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160,\n",
    "    thresholds=[0.6, 0.7, 0.7],\n",
    "#        factor=0.709,\n",
    "#        prewhiten=True,\n",
    "    keep_all=True,\n",
    "    device=torch.device('cuda')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(imgs):\n",
    "    # 2. Create a PyTorch Variable with the transformed image\n",
    "    t_img = Variable(imgs).type(dtype)  \n",
    "    \n",
    "    # 3. Create a vector of zeros that will hold our feature vector\n",
    "    #    The 'avgpool' layer has an output size of 512\n",
    "    my_embedding = torch.zeros(imgs.shape[0], 512)    \n",
    "\n",
    "    # 4. Define a function that will copy the output of a layer\n",
    "    def copy_data(m, i, o):\n",
    "      my_embedding.copy_(o.data.squeeze())    \n",
    "    \n",
    "    # 5. Attach that function to our selected layer\n",
    "    h = layer.register_forward_hook(copy_data)    \n",
    "    # 6. Run the model on our transformed image\n",
    "    resnet(t_img)    \n",
    "    # 7. Detach our copy function from the layer\n",
    "    h.remove()    \n",
    "    # 8. Return the feature vector\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativ resnet\n",
    "#vggf_resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "#img_embedding = vggf_resnet(img_cropped.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.cuda()\n",
    "# Use the model object to select the desired layer\n",
    "layer = resnet._modules.get('avgpool')\n",
    "_ = resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mistakes(pred, target):\n",
    "    #pred = pred[0]\n",
    "#    print('calculating mistakes : ')\n",
    "#    print('   pred: ' + str(pred.shape))\n",
    "#    print('   target: ' + str(target.shape))\n",
    "    \n",
    "    mistakes = 0\n",
    "    \n",
    "    if len(pred) != len(target):\n",
    "        raise Exception('sizes of both tensors must match')\n",
    "        \n",
    "    for x,y in zip(pred, target):\n",
    "        if round(x.item()) != y.item():\n",
    "            mistakes = mistakes + 1\n",
    "        \n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss() #MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=1e-5) #weight_decay=1e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Epoch : 0\n",
      "Train loss : 7185.5205078125\n",
      "Average errors : 57.55289139633286\n",
      "Valid loss : 1847.111572265625\n",
      "Average valid errors : 63.174364896073904\n",
      "======================\n",
      "Epoch : 1\n",
      "Train loss : 7173.94677734375\n",
      "Average errors : 57.33530794546309\n",
      "Valid loss : 1846.30322265625\n",
      "Average valid errors : 62.985373364126254\n",
      "======================\n",
      "Epoch : 2\n",
      "Train loss : 7152.490234375\n",
      "Average errors : 56.943864598025385\n",
      "Valid loss : 1849.8692626953125\n",
      "Average valid errors : 62.985758275596616\n",
      "======================\n",
      "Epoch : 3\n",
      "Train loss : 7140.49951171875\n",
      "Average errors : 56.745933239304186\n",
      "Valid loss : 1853.8673095703125\n",
      "Average valid errors : 62.92532717474981\n",
      "======================\n",
      "Epoch : 4\n",
      "Train loss : 7123.25732421875\n",
      "Average errors : 56.5444287729196\n",
      "Valid loss : 1856.456787109375\n",
      "Average valid errors : 62.87182448036952\n",
      "======================\n",
      "Epoch : 5\n",
      "Train loss : 7110.06201171875\n",
      "Average errors : 56.44438175834509\n",
      "Valid loss : 1855.7462158203125\n",
      "Average valid errors : 62.86374133949192\n",
      "======================\n",
      "Epoch : 6\n",
      "Train loss : 7097.13671875\n",
      "Average errors : 56.32552891396333\n",
      "Valid loss : 1854.773681640625\n",
      "Average valid errors : 62.867590454195536\n",
      "======================\n",
      "Epoch : 7\n",
      "Train loss : 7089.6220703125\n",
      "Average errors : 56.20216267042783\n",
      "Valid loss : 1855.9718017578125\n",
      "Average valid errors : 62.76943802925327\n",
      "======================\n",
      "Epoch : 8\n",
      "Train loss : 7080.259765625\n",
      "Average errors : 56.07625763986836\n",
      "Valid loss : 1857.07275390625\n",
      "Average valid errors : 62.68937644341801\n",
      "======================\n",
      "Epoch : 9\n",
      "Train loss : 7069.0068359375\n",
      "Average errors : 55.94471086036671\n",
      "Valid loss : 1856.397705078125\n",
      "Average valid errors : 62.69284064665127\n",
      "======================\n",
      "Epoch : 10\n",
      "Valid loss : 1854.70654296875\n",
      "Average valid errors : 62.63317936874519\n",
      "======================\n",
      "Epoch : 11\n",
      "Train loss : 7048.6806640625\n",
      "Average errors : 55.914527503526095\n",
      "Valid loss : 1854.122314453125\n",
      "Average valid errors : 62.58775981524249\n",
      "======================\n",
      "Epoch : 12\n",
      "Train loss : 7039.36279296875\n",
      "Average errors : 55.856323460272684\n",
      "Valid loss : 1854.4915771484375\n",
      "Average valid errors : 62.51693610469592\n",
      "======================\n",
      "Epoch : 13\n",
      "Train loss : 7027.82373046875\n",
      "Average errors : 55.69214856605548\n",
      "Valid loss : 1854.3714599609375\n",
      "Average valid errors : 62.52155504234026\n",
      "======================\n",
      "Epoch : 14\n",
      "Train loss : 7018.5849609375\n",
      "Average errors : 55.605077574047954\n",
      "Valid loss : 1855.5281982421875\n",
      "Average valid errors : 62.5504234026174\n",
      "======================\n",
      "Epoch : 15\n",
      "Train loss : 7005.71142578125\n",
      "Average errors : 55.4184297132111\n",
      "Valid loss : 1859.3800048828125\n",
      "Average valid errors : 62.54849884526559\n",
      "======================\n",
      "Epoch : 16\n",
      "Train loss : 6998.8935546875\n",
      "Average errors : 55.3093559003291\n",
      "Valid loss : 1863.711181640625\n",
      "Average valid errors : 62.51770592763664\n",
      "======================\n",
      "Epoch : 17\n",
      "Train loss : 6989.91796875\n",
      "Average errors : 55.13953925716972\n",
      "Valid loss : 1866.8980712890625\n",
      "Average valid errors : 62.366050808314085\n",
      "======================\n",
      "Epoch : 18\n",
      "Train loss : 6981.3154296875\n",
      "Average errors : 55.01730136342266\n",
      "Valid loss : 1868.3153076171875\n",
      "Average valid errors : 62.28406466512702\n",
      "======================\n",
      "Epoch : 19\n",
      "Train loss : 6971.3564453125\n",
      "Average errors : 54.931546779501645\n",
      "Valid loss : 1870.19189453125\n",
      "Average valid errors : 62.230177059276365\n",
      "======================\n",
      "Epoch : 20\n",
      "Train loss : 6961.32373046875\n",
      "Average errors : 54.83366243535496\n",
      "Valid loss : 1871.5657958984375\n",
      "Average valid errors : 62.232871439568896\n",
      "======================\n",
      "Epoch : 21\n",
      "Train loss : 6953.84375\n",
      "Average errors : 54.7989656793606\n",
      "Valid loss : 1872.84033203125\n",
      "Average valid errors : 62.147806004618936\n",
      "======================\n",
      "Epoch : 22\n",
      "Train loss : 6949.2353515625\n",
      "Average errors : 54.71067230841561\n",
      "Valid loss : 1871.1917724609375\n",
      "Average valid errors : 62.02001539645882\n",
      "======================\n",
      "Epoch : 23\n",
      "Train loss : 6941.76318359375\n",
      "Average errors : 54.608744710860364\n",
      "Valid loss : 1869.09130859375\n",
      "Average valid errors : 61.94264819091609\n",
      "======================\n",
      "Epoch : 24\n",
      "Train loss : 6933.07080078125\n",
      "Average errors : 54.50587682181476\n",
      "Valid loss : 1869.0726318359375\n",
      "Average valid errors : 61.96920708237106\n",
      "======================\n",
      "Epoch : 25\n",
      "Train loss : 6924.31884765625\n",
      "Average errors : 54.46318758815233\n",
      "Valid loss : 1870.14404296875\n",
      "Average valid errors : 61.95304080061586\n",
      "======================\n",
      "Epoch : 26\n",
      "Train loss : 6919.853515625\n",
      "Average errors : 54.44776680771039\n",
      "Valid loss : 1871.1802978515625\n",
      "Average valid errors : 61.960354118552736\n",
      "======================\n",
      "Epoch : 27\n",
      "Train loss : 6916.6865234375\n",
      "Average errors : 54.384861307005174\n",
      "Valid loss : 1873.728271484375\n",
      "Average valid errors : 62.04657428791378\n",
      "======================\n",
      "Epoch : 28\n",
      "Train loss : 6911.78076171875\n",
      "Average errors : 54.2940291490362\n",
      "Valid loss : 1876.7205810546875\n",
      "Average valid errors : 61.98229407236336\n",
      "======================\n",
      "Epoch : 29\n",
      "Train loss : 6906.0390625\n",
      "Average errors : 54.18862247296662\n",
      "Valid loss : 1879.101806640625\n",
      "Average valid errors : 61.97344110854503\n",
      "======================\n",
      "Epoch : 30\n",
      "Train loss : 6899.45703125\n",
      "Average errors : 54.04945933239304\n",
      "Valid loss : 1880.0594482421875\n",
      "Average valid errors : 61.951116243264046\n",
      "======================\n",
      "Epoch : 31\n",
      "Train loss : 6893.19384765625\n",
      "Average errors : 53.98439116125999\n",
      "Valid loss : 1879.4566650390625\n",
      "Average valid errors : 61.99961508852964\n",
      "======================\n",
      "Epoch : 32\n",
      "Train loss : 6886.19775390625\n",
      "Average errors : 53.87559943582511\n",
      "Valid loss : 1881.5743408203125\n",
      "Average valid errors : 62.20169361046959\n",
      "======================\n",
      "Epoch : 33\n",
      "Train loss : 6881.76171875\n",
      "Average errors : 53.88086506817113\n",
      "Valid loss : 1883.494384765625\n",
      "Average valid errors : 62.26250962278676\n",
      "======================\n",
      "Epoch : 34\n",
      "Train loss : 6880.20458984375\n",
      "Average errors : 53.84964739069112\n",
      "Valid loss : 1884.07373046875\n",
      "Average valid errors : 62.23595073133179\n",
      "======================\n",
      "Epoch : 35\n",
      "Train loss : 6875.1328125\n",
      "Average errors : 53.77132110954396\n",
      "Valid loss : 1884.896484375\n",
      "Average valid errors : 62.12933025404157\n",
      "======================\n",
      "Epoch : 36\n",
      "Train loss : 6871.43408203125\n",
      "Average errors : 53.74499294781382\n",
      "Valid loss : 1883.7264404296875\n",
      "Average valid errors : 62.03656658968437\n",
      "======================\n",
      "Epoch : 37\n",
      "Train loss : 6868.13427734375\n",
      "Average errors : 53.66732487070992\n",
      "Valid loss : 1884.229248046875\n",
      "Average valid errors : 62.005773672055426\n",
      "======================\n",
      "Epoch : 38\n",
      "Train loss : 6865.671875\n",
      "Average errors : 53.617771509167845\n",
      "Valid loss : 1883.9981689453125\n",
      "Average valid errors : 62.07621247113164\n",
      "======================\n",
      "Epoch : 39\n",
      "Train loss : 6862.30078125\n",
      "Average errors : 53.563422661024916\n",
      "Valid loss : 1884.525634765625\n",
      "Average valid errors : 62.09083910700539\n",
      "======================\n",
      "Epoch : 40\n",
      "Train loss : 6858.71337890625\n",
      "Average errors : 53.526563234602726\n",
      "Valid loss : 1884.8450927734375\n",
      "Average valid errors : 62.056581986143186\n",
      "======================\n",
      "Epoch : 41\n",
      "Train loss : 6853.251953125\n",
      "Average errors : 53.46770098730607\n",
      "Valid loss : 1887.158447265625\n",
      "Average valid errors : 62.03810623556582\n",
      "======================\n",
      "Epoch : 42\n",
      "Train loss : 6848.33251953125\n",
      "Average errors : 53.39031499764927\n",
      "Valid loss : 1887.4918212890625\n",
      "Average valid errors : 62.04464973056197\n",
      "======================\n",
      "Epoch : 43\n",
      "Train loss : 6845.33056640625\n",
      "Average errors : 53.34518100611189\n",
      "Valid loss : 1888.8253173828125\n",
      "Average valid errors : 61.985758275596616\n",
      "======================\n",
      "Epoch : 44\n",
      "Train loss : 6841.18017578125\n",
      "Average errors : 53.286506817113306\n",
      "Valid loss : 1889.8697509765625\n",
      "Average valid errors : 61.89222478829869\n",
      "======================\n",
      "Epoch : 45\n",
      "Train loss : 6835.0498046875\n",
      "Average errors : 53.222378937470616\n",
      "Valid loss : 1889.0675048828125\n",
      "Average valid errors : 61.80831408775982\n",
      "======================\n",
      "Epoch : 46\n",
      "Train loss : 6827.2705078125\n",
      "Average errors : 53.10296191819464\n",
      "Valid loss : 1889.168212890625\n",
      "Average valid errors : 61.82448036951501\n",
      "======================\n",
      "Epoch : 47\n",
      "Train loss : 6820.46142578125\n",
      "Average errors : 52.999341795956745\n",
      "Valid loss : 1890.0826416015625\n",
      "Average valid errors : 61.76558891454965\n",
      "======================\n",
      "Epoch : 48\n",
      "Train loss : 6811.640625\n",
      "Average errors : 52.88613070051716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss : 1890.4609375\n",
      "Average valid errors : 61.72594303310239\n",
      "======================\n",
      "Epoch : 49\n",
      "Train loss : 6802.5263671875\n",
      "Average errors : 52.7594734367654\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "for e in range(epochs):\n",
    "    print(\"======================\")\n",
    "    print(\"Epoch : \" + str(e))\n",
    "    epoch_loss = 0\n",
    "    epoch_mistakes = 0\n",
    "    train_size = len(train_dl)\n",
    "    \n",
    "    # activate train mode\n",
    "    model.train()\n",
    "\n",
    "    for index, (data, target) in enumerate(train_dl):\n",
    "        faces, prob = mtcnn(to_image(data[0]), return_prob=True)\n",
    " \n",
    " #       if faces is None:\n",
    " #           print('no face')\n",
    " #           continue\n",
    " #       else:\n",
    " #           print('face found')\n",
    "                \n",
    "        target_t = torch.cuda.FloatTensor([int(x) for x in train_ds.idx_to_class[target[0].item()]])\n",
    "\n",
    "        emb = get_vectors(faces)    \n",
    "        data_v   = Variable(emb[0], requires_grad=False).type(dtype)\n",
    "        target_v = Variable(target_t, requires_grad=False).type(dtype)\n",
    "\n",
    "        # forward\n",
    "#        print('datav: ' + str(data_v.shape))\n",
    "        pred = model.forward(data_v)\n",
    "#        print('pred: ' + str(pred.shape))\n",
    "        # zero grads\n",
    "        optimizer.zero_grad()\n",
    "        # calculate loss\n",
    "        loss = loss_func(pred, target_v.float())\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        # back prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_mistakes = epoch_mistakes + calculate_mistakes(pred, target_t)\n",
    "        \n",
    "    print(\"Train loss : \" + str(epoch_loss.item()))\n",
    "    print(\"Average errors : \" + str(epoch_mistakes/train_size))\n",
    "    \n",
    "    # activate eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_mistakes = 0\n",
    "    valid_size = len(valid_dl)\n",
    "    for index, (data, target) in enumerate(valid_dl):\n",
    "        faces, prob = mtcnn(to_image(data[0]), return_prob=True)\n",
    "        target_t = torch.FloatTensor([int(x) for x in valid_ds.idx_to_class[target[0].item()]])\n",
    "        \n",
    "        emb = get_vectors(faces) \n",
    "        data_v   = Variable(emb[0], requires_grad=False).type(dtype)\n",
    "        target_v = Variable(target_t, requires_grad=False).type(dtype)\n",
    "\n",
    "        pred = model.forward(data_v)\n",
    "        loss = loss_func(pred, target_v.float())\n",
    "        valid_loss = valid_loss + loss\n",
    "        valid_mistakes = valid_mistakes + calculate_mistakes(pred, target_t)\n",
    "        \n",
    "        \n",
    "    print(\"Valid loss : \" + str(valid_loss.item()))\n",
    "    print(\"Average valid errors : \" + str(valid_mistakes/valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'dl128b_v1_400.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
